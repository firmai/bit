<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2018-10-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lime.html">
<link rel="next" href="example-based.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b> Storytime</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="2.3" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>2.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b> The Importance of Interpretability</a></li>
<li class="chapter" data-level="3.2" data-path="criteria-for-interpretability-methods.html"><a href="criteria-for-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b> Criteria for Interpretability Methods</a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>3.4</b> Evaluating Interpretability</a></li>
<li class="chapter" data-level="3.5" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.5</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="3.5.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.5.1</b> What is an explanation?</a></li>
<li class="chapter" data-level="3.5.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.5.2</b> What is a “good” explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Datasets</a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b> Bike Sharing Counts (Regression)</a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable Models</a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> Linear Model</a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>5.1.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>5.1.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>5.1.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>5.1.5</b> Explaining Single Predictions</a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.6</b> Coding Categorical Features</a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>5.1.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.8</b> Do linear models create good explanations?</a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#extending-linear-models"><i class="fa fa-check"></i><b>5.1.9</b> Extending Linear Models</a></li>
<li class="chapter" data-level="5.1.10" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.10</b> Sparse linear models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#whats-wrong-with-linear-regression-models-for-classification"><i class="fa fa-check"></i><b>5.2.1</b> What’s Wrong with Linear Regression Models for Classification?</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example"><i class="fa fa-check"></i><b>5.2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>5.3</b> Decision Tree</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>5.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.3.2" data-path="tree.html"><a href="tree.html#interpretation-example-1"><i class="fa fa-check"></i><b>5.3.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="5.3.3" data-path="tree.html"><a href="tree.html#advantages"><i class="fa fa-check"></i><b>5.3.3</b> Advantages</a></li>
<li class="chapter" data-level="5.3.4" data-path="tree.html"><a href="tree.html#disadvantages"><i class="fa fa-check"></i><b>5.3.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>5.4</b> Decision Rules (IF-THEN)</a><ul>
<li class="chapter" data-level="5.4.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>5.4.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="5.4.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>5.4.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="5.4.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>5.4.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="5.4.4" data-path="rules.html"><a href="rules.html#advantages-1"><i class="fa fa-check"></i><b>5.4.4</b> Advantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="rules.html"><a href="rules.html#disadvantages-1"><i class="fa fa-check"></i><b>5.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>5.4.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>5.5</b> RuleFit</a><ul>
<li class="chapter" data-level="5.5.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>5.5.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="5.5.2" data-path="rulefit.html"><a href="rulefit.html#guidelines"><i class="fa fa-check"></i><b>5.5.2</b> Guidelines</a></li>
<li class="chapter" data-level="5.5.3" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>5.5.3</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>5.6</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="5.6.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.6.1</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="5.6.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.6.2</b> K-Nearest Neighbours</a></li>
<li class="chapter" data-level="5.6.3" data-path="other-interpretable.html"><a href="other-interpretable.html#and-so-many-more"><i class="fa fa-check"></i><b>5.6.3</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>6.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>6.1.1</b> Examples</a></li>
<li class="chapter" data-level="6.1.2" data-path="pdp.html"><a href="pdp.html#advantages-2"><i class="fa fa-check"></i><b>6.1.2</b> Advantages</a></li>
<li class="chapter" data-level="6.1.3" data-path="pdp.html"><a href="pdp.html#disadvantages-2"><i class="fa fa-check"></i><b>6.1.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>6.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ice.html"><a href="ice.html#example-1"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
<li class="chapter" data-level="6.2.2" data-path="ice.html"><a href="ice.html#advantages-3"><i class="fa fa-check"></i><b>6.2.2</b> Advantages</a></li>
<li class="chapter" data-level="6.2.3" data-path="ice.html"><a href="ice.html#disadvantages-3"><i class="fa fa-check"></i><b>6.2.3</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>6.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="6.3.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>6.3.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="6.3.2" data-path="ale.html"><a href="ale.html#theory-1"><i class="fa fa-check"></i><b>6.3.2</b> Theory</a></li>
<li class="chapter" data-level="6.3.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>6.3.3</b> Estimation</a></li>
<li class="chapter" data-level="6.3.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>6.3.4</b> Examples</a></li>
<li class="chapter" data-level="6.3.5" data-path="ale.html"><a href="ale.html#advantages-4"><i class="fa fa-check"></i><b>6.3.5</b> Advantages</a></li>
<li class="chapter" data-level="6.3.6" data-path="ale.html"><a href="ale.html#disadvantages-4"><i class="fa fa-check"></i><b>6.3.6</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>6.3.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>6.4</b> Feature Interaction</a><ul>
<li class="chapter" data-level="6.4.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>6.4.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="6.4.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>6.4.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="6.4.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>6.4.3</b> Examples</a></li>
<li class="chapter" data-level="6.4.4" data-path="interaction.html"><a href="interaction.html#advantages-5"><i class="fa fa-check"></i><b>6.4.4</b> Advantages</a></li>
<li class="chapter" data-level="6.4.5" data-path="interaction.html"><a href="interaction.html#disadvantages-5"><i class="fa fa-check"></i><b>6.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="6.4.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>6.4.6</b> Implementations</a></li>
<li class="chapter" data-level="6.4.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>6.4.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>6.5</b> Feature Importance</a><ul>
<li class="chapter" data-level="6.5.1" data-path="feature-importance.html"><a href="feature-importance.html#the-theory"><i class="fa fa-check"></i><b>6.5.1</b> The Theory</a></li>
<li class="chapter" data-level="6.5.2" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>6.5.2</b> Example and Interpretation</a></li>
<li class="chapter" data-level="6.5.3" data-path="feature-importance.html"><a href="feature-importance.html#advantages-6"><i class="fa fa-check"></i><b>6.5.3</b> Advantages</a></li>
<li class="chapter" data-level="6.5.4" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-6"><i class="fa fa-check"></i><b>6.5.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>6.6</b> Global Surrogate Models</a><ul>
<li class="chapter" data-level="6.6.1" data-path="global.html"><a href="global.html#theory-2"><i class="fa fa-check"></i><b>6.6.1</b> Theory</a></li>
<li class="chapter" data-level="6.6.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>6.6.2</b> Example</a></li>
<li class="chapter" data-level="6.6.3" data-path="global.html"><a href="global.html#advantages-7"><i class="fa fa-check"></i><b>6.6.3</b> Advantages</a></li>
<li class="chapter" data-level="6.6.4" data-path="global.html"><a href="global.html#disadvantages-7"><i class="fa fa-check"></i><b>6.6.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>6.7</b> Local Surrogate Models (LIME)</a><ul>
<li class="chapter" data-level="6.7.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>6.7.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="6.7.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>6.7.2</b> LIME for Text</a></li>
<li class="chapter" data-level="6.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>6.7.3</b> LIME for Images</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>6.8</b> Shapley Value Explanations</a><ul>
<li class="chapter" data-level="6.8.1" data-path="shapley.html"><a href="shapley.html#the-general-idea"><i class="fa fa-check"></i><b>6.8.1</b> The general idea</a></li>
<li class="chapter" data-level="6.8.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>6.8.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="6.8.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>6.8.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="6.8.4" data-path="shapley.html"><a href="shapley.html#advantages-8"><i class="fa fa-check"></i><b>6.8.4</b> Advantages</a></li>
<li class="chapter" data-level="6.8.5" data-path="shapley.html"><a href="shapley.html#disadvantages-8"><i class="fa fa-check"></i><b>6.8.5</b> Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>7</b> Example-based Explanations</a><ul>
<li class="chapter" data-level="7.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>7.1</b> Counterfactual explanations</a><ul>
<li class="chapter" data-level="7.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>7.1.1</b> Generating counterfactual explanations</a></li>
<li class="chapter" data-level="7.1.2" data-path="counterfactual.html"><a href="counterfactual.html#examples-3"><i class="fa fa-check"></i><b>7.1.2</b> Examples</a></li>
<li class="chapter" data-level="7.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-9"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-9"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>7.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>7.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="7.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>7.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="7.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>7.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>7.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="proto.html"><a href="proto.html#theory-3"><i class="fa fa-check"></i><b>7.3.1</b> Theory</a></li>
<li class="chapter" data-level="7.3.2" data-path="proto.html"><a href="proto.html#examples-4"><i class="fa fa-check"></i><b>7.3.2</b> Examples</a></li>
<li class="chapter" data-level="7.3.3" data-path="proto.html"><a href="proto.html#advantages-10"><i class="fa fa-check"></i><b>7.3.3</b> Advantages</a></li>
<li class="chapter" data-level="7.3.4" data-path="proto.html"><a href="proto.html#disadvantages-10"><i class="fa fa-check"></i><b>7.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>7.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>7.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="7.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>7.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="7.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>7.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="7.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>7.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="7.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>7.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="7.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>7.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute</a></li>
<li class="chapter" data-level="10" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>10</b> Citation</a></li>
<li class="chapter" data-level="11" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>11</b> Acknowledgements</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shapley" class="section level2">
<h2><span class="header-section-number">6.8</span> Shapley Value Explanations</h2>
<p>Predictions can be explained by assuming that each feature is a ‘player’ in a game where the prediction is the payout.
The Shapley value - a method from coalitional game theory - tells us how to fairly distribute the ‘payout’ among the features.</p>
<div id="the-general-idea" class="section level3">
<h3><span class="header-section-number">6.8.1</span> The general idea</h3>
<p>Assume the following scenario:</p>
<p>You trained a machine learning model to predict apartment prices.
For a certain apartment it predicts 300,000 € and you need to explain this prediction.
The apartment has a size of 50 m<sup>2</sup>, is located on the 2nd floor, with a park nearby and cats are forbidden:</p>
<div class="figure"><span id="fig:shapley-instance"></span>
<img src="images/shapley-instance.png" alt="The predicted price for our apartment is 300,000€. It's a 50 square meter apartment on the second floor. There is a park nearby and cats are forbidden. Our goal is to explain how each of these features values contributed towards the predicted price of 300k€."  />
<p class="caption">
FIGURE 6.32: The predicted price for our apartment is 300,000€. It’s a 50 square meter apartment on the second floor. There is a park nearby and cats are forbidden. Our goal is to explain how each of these features values contributed towards the predicted price of 300k€.
</p>
</div>
<p>The average prediction for all apartments is 310,000€.
How much did each feature value contribute to the prediction compared to the average prediction?</p>
<p>The answer is easy for linear regression models:
The effect of each feature is the weight of the feature times the feature value minus the average effect of all apartments:
This works only because of the linearity of the model.
For more complex model we need a different solution.
For example <a href="lime.html#lime">LIME</a> suggests local models to estimate effects.</p>
<p>A different solution comes from cooperative game theory:
The Shapley value, coined by Shapley (1953)<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>, is a method for assigning payouts to players depending on their contribution towards the total payout.
Players cooperate in a coalition and obtain a certain gain from that cooperation.</p>
<p>Players?
Game?
Payout?
What’s the connection to machine learning prediction and interpretability?
The ‘game’ is the prediction task for a single instance of the dataset.
The ‘gain’ is the actual prediction for this instance minus the average prediction of all instances.
The ‘players’ are the feature values of the instance, which collaborate to receive the gain (= predict a certain value).
In our apartment example, the feature values ‘park-allowed’, ‘cat-forbidden’, ‘area-50m<sup>2</sup>’ and ‘floor-2nd’ worked together to achieve the prediction of 300,000€.
Our goal is to explain the difference of the actual prediction (300,000€) and the average prediction (310,000€): a difference of -10,000€.</p>
<p>The answer might be:
The ‘park-nearby’ contributed 30,000€; ‘size-50m<sup>2</sup>’ contributed 10,000€; ‘floor-2nd’ contributed 0€; ‘cat-forbidden’ contributed -50,000€.
The contributions add up to -10,000€: the final prediction minus the average predicted apartment price.</p>
<p><strong>How do we calculate the Shapley value for one feature?</strong></p>
<p>The Shapley value is the average marginal contribution of a feature value over all possible coalitions.
All clear now?
How to compute the contribution of a feature value to a coalition:</p>
<div class="figure"><span id="fig:shapley-instance-intervened"></span>
<img src="images/shapley-instance-intervention.png" alt="We assess the contribution of the 'cat-forbidden' feature value when added to a coalition of 'park-nearby', 'size-50m^2^'. We simulate that only 'park-nearby', 'cat-forbidden' and 'size-50m^2^' are in a coalition by randomly drawing the value for the floor feature. Then we predict the price of the apartment with this combination (310,000€). In a second step we remove 'cat-forbidden' from the coalition by replacing it with a random value of the cat allowed/forbidden feature from the randomly drawn apartment. In the example it was 'cat-allowed', but it could have been 'cat-forbidden' again. We predict the apartment price for the coalition of 'park-nearby' and 'size-50m^2^' (320,000€). The contribution of 'cat-forbidden' was 310,000€ - 320,000€ = -10.000€. This estimation depends on the sampled non-participating feature values and we get better estimates by repeating this procedure. This figure shows the computation of the marginal contribution for only one coalition. The Shapley value is the weighted average of marginal contributions over all coalitions."  />
<p class="caption">
FIGURE 6.33: We assess the contribution of the ‘cat-forbidden’ feature value when added to a coalition of ‘park-nearby’, ‘size-50m<sup>2</sup>’. We simulate that only ‘park-nearby’, ‘cat-forbidden’ and ‘size-50m<sup>2</sup>’ are in a coalition by randomly drawing the value for the floor feature. Then we predict the price of the apartment with this combination (310,000€). In a second step we remove ‘cat-forbidden’ from the coalition by replacing it with a random value of the cat allowed/forbidden feature from the randomly drawn apartment. In the example it was ‘cat-allowed’, but it could have been ‘cat-forbidden’ again. We predict the apartment price for the coalition of ‘park-nearby’ and ‘size-50m<sup>2</sup>’ (320,000€). The contribution of ‘cat-forbidden’ was 310,000€ - 320,000€ = -10.000€. This estimation depends on the sampled non-participating feature values and we get better estimates by repeating this procedure. This figure shows the computation of the marginal contribution for only one coalition. The Shapley value is the weighted average of marginal contributions over all coalitions.
</p>
</div>
<p>We repeat this computation for all possible coalitions.
The computation time increases exponentially with the number of features, so we have to sample from all possible coalitions.
The Shapley value is the average over all the marginal contributions.
Here are all coalitions for computing the Shapley value of the ‘cat-forbidden’ feature value:</p>
<div class="figure"><span id="fig:shapley-coalitions"></span>
<img src="images/shapley-coalitions.png" alt="All coalitions of feature values that are needed to assess the Shapley value for 'cat-forbidden'. The first row shows the coalition without any feature values. The 2nd, 3rd and 4th row show different coalitions - separated by '|' - with increasing coalition size. For each of those coalitions we compute the predicted apartment price with and without the 'cat-forbidden' feature value and take the difference to get the marginal contribution. The Shapley value is the (weighted) average of marginal contributions. We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model."  />
<p class="caption">
FIGURE 6.34: All coalitions of feature values that are needed to assess the Shapley value for ‘cat-forbidden’. The first row shows the coalition without any feature values. The 2nd, 3rd and 4th row show different coalitions - separated by ‘|’ - with increasing coalition size. For each of those coalitions we compute the predicted apartment price with and without the ‘cat-forbidden’ feature value and take the difference to get the marginal contribution. The Shapley value is the (weighted) average of marginal contributions. We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model.
</p>
</div>
<p>When we repeat the Shapley value for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values.</p>
</div>
<div id="examples-and-interpretation" class="section level3">
<h3><span class="header-section-number">6.8.2</span> Examples and Interpretation</h3>
<p>The interpretation of the Shapley value <span class="math inline">\(\phi_{ij}\)</span> for feature <span class="math inline">\(j\)</span> and instance <span class="math inline">\(i\)</span> is: the feature value <span class="math inline">\(x_{ij}\)</span> contributed <span class="math inline">\(\phi_{ij}\)</span> towards the prediction for instance <span class="math inline">\(i\)</span> compared to the average prediction for the dataset.</p>
<p>The Shapley value works for both classification (if we deal with probabilities) and regression.</p>
<p>We use the Shapley value to analyse the predictions of a Random Forest model predicting <a href="cervical.html#cervical">cervical cancer</a>:</p>
<div class="figure"><span id="fig:shapley-cervical-plot"></span>
<img src="images/shapley-cervical-plot-1.png" alt="Feature value contributions for woman 326 in the cervical cancer dataset. With a prediction of 0.43, this woman's cancer probability is 0.41 above the average prediction of 0.03. The feature value that increased the probability the most is the number of diagnosed STDs. The feature contributions sum up to the difference of actual and average prediction (0.41)." width="1050" />
<p class="caption">
FIGURE 6.35: Feature value contributions for woman 326 in the cervical cancer dataset. With a prediction of 0.43, this woman’s cancer probability is 0.41 above the average prediction of 0.03. The feature value that increased the probability the most is the number of diagnosed STDs. The feature contributions sum up to the difference of actual and average prediction (0.41).
</p>
</div>
<p>For the <a href="bike-data.html#bike-data">bike rental dataset</a> we also train a Random Forest to predict the number of rented bikes for a day given the weather conditions and calendric information.
The explanations created for the Random Forest prediction of one specific day:</p>
<div class="figure"><span id="fig:shapley-bike-plot"></span>
<img src="images/shapley-bike-plot-1.png" alt="Feature value contributions for instance 285. With a predicted 2329 rented bikes, this day is -2189 below the average prediction of 4517. The feature values that had the most negative effects were the weather situation, humidity and the time trend (years since 2011). The temperature on that day had a positive effect compared to the average prediction. The feature contributions sum up to the difference of actual and average prediction (-2189)." width="1050" />
<p class="caption">
FIGURE 6.36: Feature value contributions for instance 285. With a predicted 2329 rented bikes, this day is -2189 below the average prediction of 4517. The feature values that had the most negative effects were the weather situation, humidity and the time trend (years since 2011). The temperature on that day had a positive effect compared to the average prediction. The feature contributions sum up to the difference of actual and average prediction (-2189).
</p>
</div>
<p>Be careful to interpret the Shapley value correctly:
The Shapley value is the average contribution of a feature value towards the prediction in different coalitions.
The Shapley value is NOT the difference in prediction when we would drop the feature from the model.</p>
</div>
<div id="the-shapley-value-in-detail" class="section level3">
<h3><span class="header-section-number">6.8.3</span> The Shapley Value in Detail</h3>
<p>This Section goes deeper into the definition and computation of the Shapley value for the curious reader.
Skip this part straight to ‘Advantages and Disadvantages’ if you are not interested in the technicalities.</p>
<p>We are interested in the effect each feature has on the prediction of a data point.
In a linear model it is easy to calculate the individual effects.
Here’s how a linear model prediction looks like for one data instance:</p>
<p><span class="math display">\[\hat{f}(x_{i\cdot})=\hat{f}(x_{i1},\ldots,x_{ip})=\beta_0+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}\]</span></p>
<p>where <span class="math inline">\(x_{i\cdot}\)</span> is the instance for which we want to compute the feature effects.
Each <span class="math inline">\(x_{ij}\)</span> is a feature value, with <span class="math inline">\(j\in\{1,\ldots,p\}\)</span>.
The <span class="math inline">\(\beta_j\)</span> are the weights corresponding to <span class="math inline">\(x_{ij}\)</span>.</p>
<p>The feature effect <span class="math inline">\(\phi_{ij}\)</span> of <span class="math inline">\(x_{ij}\)</span> on the prediction <span class="math inline">\(\hat{f}(x_{i\cdot})\)</span> is:</p>
<p><span class="math display">\[\phi_{ij}(\hat{f})=\beta_{j}x_{ij}-E(\beta_{j}X_{j})=\beta_{j}x_{ij}-\beta_{j}E(X_{j})\]</span></p>
<p>where <span class="math inline">\(E(\beta_jX_{j})\)</span> is the mean effect estimate for feature <span class="math inline">\(X_{j}\)</span>.
The effect is the difference between the feature contribution to the equation minus the average contribution.
Nice!
Now we know how much each feature contributed towards the prediction.
If we sum up all the feature effects over all features for one instance, the result is:</p>
<p><span class="math display">\[\sum_{j=1}^{p}\phi_{ij}(\hat{f})=\sum_{j=1}^p(\beta_{j}x_{ij}-E(\beta_{j}X_{j}))=(\beta_0+\sum_{j=1}^p\beta_{j}x_{ij})-(\beta_0+\sum_{j=1}^{p}E(\beta_{j}X_{j}))=\hat{f}(x_{i\cdot})-E(\hat{f}(X))\]</span></p>
<p>This is the predicted value for the data point <span class="math inline">\(x_{i\cdot}\)</span> minus the average predicted value.
Feature effects <span class="math inline">\(\phi_{ij}\)</span> can be negative.</p>
<p>Now, can we do the same for any type of model?
It would be great to have this as a model-agnostic tool.
Since we don’t have the <span class="math inline">\(\beta\)</span>’s from a linear equation in other model types, we need a different solution.</p>
<p>Help comes from unexpected places: cooperative game theory.
The Shapley value is a solution for computing feature effects <span class="math inline">\(\phi_{ij}(\hat{f})\)</span> for single predictions for any machine learning model <span class="math inline">\(\hat{f}\)</span>.</p>
<div id="the-shapley-value" class="section level4">
<h4><span class="header-section-number">6.8.3.1</span> The Shapley Value</h4>
<p>The Shapley value is defined via a value function <span class="math inline">\(val\)</span> over players in S.</p>
<p>The Shapley value of a feature value <span class="math inline">\(x_{ij}\)</span> is it’s contribution to the payed outcome, weighted and summed over all possible feature value combinations:</p>
<p><span class="math display">\[\phi_{ij}(val)=\sum_{S\subseteq\{x_{i1},\ldots,x_{ip}\}\setminus\{x_{ij}\}}\frac{|S|!\left(p-|S|-1\right)!}{p!}\left(val\left(S\cup\{x_{ij}\}\right)-val(S)\right)\]</span></p>
<p>where <span class="math inline">\(S\)</span> is a subset of the features used in the model, <span class="math inline">\(x_{i\cdot}\)</span> is the vector feature values of instance <span class="math inline">\(i\)</span> and <span class="math inline">\(p\)</span> the number of features.
<span class="math inline">\(val_{x_i}(S)\)</span> is the prediction for feature values in set <span class="math inline">\(S\)</span>, marginalised over features not in <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[val_{x_i}(S)=\int\hat{f}(x_{i1},\ldots,x_{ip})d\mathbb{P}_{X_{i\cdot}\notin{}S}-E_X(\hat{f}(X))\]</span></p>
<p>You actually do multiple integrations, for each feature not in <span class="math inline">\(S\)</span>.
One concrete example:
The machine learning model works on 4 features <span class="math inline">\(\{x_{i1},x_{i2},x_{i3},x_{i4}\}\)</span> and we evaluate <span class="math inline">\(\hat{f}\)</span> for the coalition <span class="math inline">\(S\)</span> consisting of feature values <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i3}\)</span>:</p>
<p><span class="math display">\[val_{x_i}(S)=val_{x_i}(\{x_{i1},x_{i3}\})=\int_{\mathbb{R}}\int_{\mathbb{R}}\hat{f}(x_{i1},X_{2},x_{i3},X_{4})d\mathbb{P}_{X_2,X_4}-E_X(\hat{f}(X))\]</span></p>
<p>This looks similar to the linear model feature effects!</p>
<p>Don’t get confused by the many uses of the word ‘value’:
The feature value is the numerical value of a feature and instance;
the Shapley value is the feature contribution towards the prediction;
the value function is the payout function given a certain coalition of players (feature values).</p>
<p>The Shapley value is the only attribution method that satisfies the following properties (which can be seen as a definition of a fair payout):</p>
<ol style="list-style-type: decimal">
<li><strong>Efficiency</strong>: <span class="math inline">\(\sum\nolimits_{j=1}^p\phi_{ij}=\hat{f}(x_i)-E_X(\hat{f}(X))\)</span>.
The feature effects have to sum up to the difference of prediction for <span class="math inline">\(x_{i\cdot}\)</span> and the average.</li>
<li><strong>Symmetry</strong>: If <span class="math inline">\(val(S\cup\{x_{ij}\})=val(S\cup\{x_{ik}\})\)</span> for all <span class="math inline">\(S\subseteq\{x_{i1},\ldots,x_{ip}\}\setminus\{x_{ij},x_{ik}\}\)</span>, then <span class="math inline">\(\phi_{ij}=\phi_{ik}\)</span>.
The contribution for two features should be the same if they contribute equally to all possible coalitions.</li>
<li><strong>Dummy</strong>: If <span class="math inline">\(val(S\cup\{x_{ij}\})=val(S)\)</span> for all <span class="math inline">\(S\subseteq\{x_{i1},\ldots,x_{ip}\}\)</span>, then <span class="math inline">\(\phi_{ij}=0\)</span>.
A feature which does not change the predicted value - no matter to which coalition of feature values it is added - should have a Shapley value of 0.</li>
<li><strong>Additivity</strong>: For a game with combined payouts <span class="math inline">\(val+val^*\)</span> the respective Shapley values are <span class="math inline">\(\phi_{ij}+\phi_{ij}^*\)</span>.
The additivity axiom has no practical relevance in the context of feature effects.</li>
</ol>
<p>An intuitive way to understand the Shapley value is the following illustration:
The feature values enter a room in random order.
All feature values in the room participate in the game (= contribute to the prediction).
The Shapley value <span class="math inline">\(\phi_{ij}\)</span> is the average marginal contribution of feature value <span class="math inline">\(x_{ij}\)</span> by joining whatever features already entered the room before, i.e.</p>
<p><span class="math display">\[\phi_{ij}=\sum_{\text{All.orderings}}val(\{\text{features.before.j}\}\cup{}x_{ij})-val(\{\text{features.before.j}\})\]</span></p>
</div>
<div id="estimating-the-shapley-value" class="section level4">
<h4><span class="header-section-number">6.8.3.2</span> Estimating the Shapley value</h4>
<p>All possible coalitions (sets) of features have to be evaluated, with and without the feature of interest for calculating the exact Shapley value for one feature value.
For more than a few features, the exact solution to this problem becomes intractable, because the number of possible coalitions increases exponentially by adding more features.
Strumbelj et al. (2014)<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> suggest an approximation with Monte-Carlo sampling:</p>
<p><span class="math display">\[\hat{\phi}_{ij}=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}(x^{*+j})-\hat{f}(x^{*-j})\right)\]</span></p>
<p>where <span class="math inline">\(\hat{f}(x^{*+j})\)</span> is the prediction for <span class="math inline">\(x_{i\cdot}\)</span>, but with a random number of features values replaced by feature values from a random data point <span class="math inline">\(x\)</span>, excluding the feature value for <span class="math inline">\(x_{ij}\)</span>
The x-vector <span class="math inline">\(x^{*-j}\)</span> is almost identical to <span class="math inline">\(x^{*+j}\)</span>, but the value <span class="math inline">\(x_{ij}\)</span> is also taken from the sampled <span class="math inline">\(x\)</span>.
Each of those <span class="math inline">\(M\)</span> new instances are kind of ‘Frankensteins’, pieced together from two instances.</p>
<p><strong>Approximate Shapley Estimation Algorithm</strong>: Each feature value <span class="math inline">\(x_{ij}\)</span>’s contribution towards the difference <span class="math inline">\(\hat{f}(x_{i\cdot})-\mathbb{E}(\hat{f})\)</span> for instance <span class="math inline">\(x_{i\cdot}\in{}X\)</span>.</p>
<ul>
<li>Require: Number of iterations <span class="math inline">\(M\)</span>, instance of interest <span class="math inline">\(x\)</span>, data <span class="math inline">\(X\)</span>, and machine learning model <span class="math inline">\(\hat{f}\)</span></li>
<li>For all <span class="math inline">\(j\in\{1,\ldots,p\}\)</span>:
<ul>
<li>For all <span class="math inline">\(m\in\{1,\ldots,M\}\)</span>:
<ul>
<li>draw random instance <span class="math inline">\(z\)</span> from <span class="math inline">\(X\)</span></li>
<li>choose a random permutation of feature <span class="math inline">\(o \in \pi(S)\)</span></li>
<li>order instance <span class="math inline">\(x\)</span>: <span class="math inline">\(x_{o}=(x_{o_1},\ldots,x_{o_j},\ldots,x_{o_p})\)</span></li>
<li>order instance <span class="math inline">\(z\)</span>: <span class="math inline">\(z_{o}=(z_{o_1},\ldots,z_{o_j},\ldots,z_{o_p})\)</span></li>
<li>construct two new instances
- <span class="math inline">\(x^{*+j}=(x_{o_1},\ldots,x_{o_{j-1}},x_{o_j},z_{o_{j+1}},\ldots,z_{o_p})\)</span>
- <span class="math inline">\(x^{*-j}=(x_{o_1},\ldots,x_{o_{j-1}},z_{o_j},z_{o_{j+1}},\ldots,z_{o_p})\)</span></li>
<li><span class="math inline">\(\phi_{ij}^{(m)}=\hat{f}(x^{*+j})-\hat{f}(x^{*-j})\)</span></li>
</ul></li>
<li>Compute the Shapley value as the average: <span class="math inline">\(\phi_{ij}(x)=\frac{1}{M}\sum_{m=1}^M\phi_{ij}^{(m)}\)</span></li>
</ul></li>
</ul>
<p>First, select an instance of interest <span class="math inline">\(i\)</span>, a feature <span class="math inline">\(j\)</span> and the number of samples <span class="math inline">\(M\)</span>.
For each sample, a random instance from the data is chosen and the order of the features is mixed.
From this instance, two new instances are created, by combining values from the instance of interest <span class="math inline">\(x\)</span> and the sample.
The first instance <span class="math inline">\(x^{*+j}\)</span> is the instance of interest, but where all values in order before and including feature <span class="math inline">\(j\)</span> are replaced by feature values from the sample.
The second instance <span class="math inline">\(x^{*-j}\)</span> is similar, but has all the values in order before, but excluding feature <span class="math inline">\(j\)</span>, replaced by features from the sample.
The difference in prediction from the black box is computed:</p>
<p><span class="math display">\[\phi_{ij}^{(m)}=\hat{f}(x^{*+j})-\hat{f}(x^{*-j})\]</span></p>
<p>All these differences are averaged and result in</p>
<p><span class="math display">\[\phi_{ij}(x)=\frac{1}{M}\sum_{m=1}^M\phi_{ij}^{(m)}\]</span></p>
<p>Averaging implicitly weighs samples by the probability distribution of <span class="math inline">\(X\)</span>.</p>
</div>
</div>
<div id="advantages-8" class="section level3">
<h3><span class="header-section-number">6.8.4</span> Advantages</h3>
<ul>
<li>The difference between the prediction and the average prediction is fairly distributed among the features values of the instance - the shapley efficiency property.
This property sets the Shapley value apart from other methods like <a href="lime.html#lime">LIME</a>.
LIME does not guarantee to perfectly distribute the effects.
It might make the Shapley value the only method to deliver a full explanation.
In situations that demand explainability by law - like EU’s “right to explanations” - the Shapley value might actually be the only compliant method.
I am not a lawyer, so this reflects only my intuition about the requirements.</li>
<li>The Shapley value allows contrastive explanations:
Instead of comparing a prediction with the average prediction of the whole dataset, you could compare it to a subset or even to a single datapoint.
This contrastiveness is also something that local models like LIME don’t have.</li>
<li>The Shapley value is the only explanation method with a solid theory.
The axioms - efficiency, symmetry, dummy, additivity - give the explanation a reasonable foundation.
Methods like LIME assume linear behaviour of the machine learning model locally but there is no theory why this should work or not.</li>
<li>It’s mind-blowing to explain a prediction as a game played by the feature values.</li>
</ul>
</div>
<div id="disadvantages-8" class="section level3">
<h3><span class="header-section-number">6.8.5</span> Disadvantages</h3>
<ul>
<li>The Shapley value needs a lot of computation time.
In 99.9% of the real world problems the approximate solution - not the exact one - is feasible.
An accurate computation of the Shapley value is potentially computational expensive, because there are <span class="math inline">\(2^k\)</span> possible coalitions of features and the ‘absence’ of a feature has to be simulated by drawing random samples, which increases the variance for the estimate <span class="math inline">\(\phi_{ij}\)</span>.
The exponential number of the coalitions is handled by sampling coalitions and fixing the number of samples <span class="math inline">\(M\)</span>.
Decreasing <span class="math inline">\(M\)</span> reduces computation time, but increases the variance of <span class="math inline">\(\phi_{ij}\)</span>.
It is unclear how to choose a sensitive <span class="math inline">\(M\)</span>.</li>
<li>The Shapley value can be misinterpreted:
The Shapley value <span class="math inline">\(\phi_{ij}\)</span> of a feature <span class="math inline">\(j\)</span> is not the difference in predicted value after the removal of feature <span class="math inline">\(j\)</span>.
The interpretation of the Shapley value is rather:
Given the current set of feature values, the total contribution of feature value <span class="math inline">\(x_{ij}\)</span> to the difference in the actual prediction and the mean prediction is <span class="math inline">\(\phi_{ij}\)</span>.</li>
<li>The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that involve only a few features).
Explanations created with the Shapley value method always use all the features.
Humans prefer selective explanations, like LIME produces.
Especially for explanations facing lay-persons, LIME might be the better choice for feature effects computation.
Another solution is <a href="https://github.com/slundberg/shap">SHAP</a> introduced by Lundberg and Lee (2016)<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>, which is based on the Shapley value, but can also produce explanations with few features.</li>
<li>The Shapley value returns a simple value per feature, and not a prediction model like LIME.
This means it can’t be used to make statements about changes in the prediction for changes in the input like:
“If I would earn 300 € more per year, my credit score should go up by 5 points.”</li>
<li>Another disadvantage is that you need access to the data if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances from the data.
This can only be avoided if you have a generator method that can create new data instances that look like real data instances but are not actual instances from the training data.
Then you could use the generator instead of the real data.</li>
</ul>

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="32">
<li id="fn32"><p>Shapley, Lloyd S. 1953. “A Value for N-Person Games.” Contributions to the Theory of Games 2 (28): 307–17.<a href="shapley.html#fnref32" class="footnote-back">↩</a></p></li>
<li id="fn33"><p>Strumbelj, Erik, Igor Kononenko, Erik Štrumbelj, and Igor Kononenko. 2014. “Explaining prediction models and individual predictions with feature contributions.” Knowledge and Information Systems 41 (3): 647–65. <a href="doi:10.1007/s10115-013-0679-x" class="uri">doi:10.1007/s10115-013-0679-x</a>.<a href="shapley.html#fnref33" class="footnote-back">↩</a></p></li>
<li id="fn34"><p>Lundberg, Scott, and Su-In Lee. 2016. “An unexpected unity among methods for interpreting model predictions,” no. Nips: 1–6. <a href="http://arxiv.org/abs/1611.07478" class="uri">http://arxiv.org/abs/1611.07478</a>.<a href="shapley.html#fnref34" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lime.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="example-based.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/05.9-agnostic-shapley.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
